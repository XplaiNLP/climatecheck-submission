{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78e0fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference with fine tuned bert-based model\"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model_path = \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "with open(\"label_mapping.json\") as f:\n",
    "    label_map = json.load(f)\n",
    "\n",
    "def predict_label(claim, abstract):\n",
    "    text = claim + \"[SEP]\" + abstract\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=512)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "        prediction = torch.argmax(outputs.logits, dim=1).item()\n",
    "\n",
    "    return label_map[str(prediction)]\n",
    "\n",
    "predictions_df = pd.read_csv(\"./predictions_best.csv\")\n",
    "test_df = pd.read_parquet(\"test-00000-of-00001.parquet\")\n",
    "abstracts_df = pd.read_parquet(\"climatecheck_publications_corpus.parquet\")\n",
    "\n",
    "claim_map = test_df.set_index(\"claim_id\")[\"claim\"].to_dict()\n",
    "abstract_map = abstracts_df.set_index(\"abstract_id\")[\"abstract\"].to_dict()\n",
    "\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "labels = []\n",
    "for _, row in tqdm(predictions_df.iterrows(), total=len(predictions_df), desc=\"Predicting labels\"):\n",
    "    claim = claim_map[row[\"claim_id\"]]\n",
    "    abstract = abstract_map[row[\"abstract_id\"]]\n",
    "    label = predict_label(claim, abstract)\n",
    "    labels.append(label)\n",
    "\n",
    "end_time = time.time()\n",
    "time_delta_seconds = end_time - start_time\n",
    "print(f\"Time delta (seconds): {time_delta_seconds}\")\n",
    "filename = \"time_log_deberta.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {start_time}\\n\")\n",
    "    f.write(f\"End Time: {end_time}\\n\")\n",
    "    f.write(f\"Time Delta (seconds): {time_delta_seconds}\\n\")\n",
    "\n",
    "predictions_df[\"label\"] = labels\n",
    "predictions_df.to_csv(\"predictions_new.csv\", index=False)\n",
    "print(\"Saved predictions with labels to predictions_new.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d99e493",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference with Phi 4 model\"\"\"\n",
    "import pandas as pd\n",
    "import json\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from vllm import LLM, SamplingParams\n",
    "import time\n",
    "\n",
    "model_path = \"microsoft/phi-4\"\n",
    "#model_path = \"microsoft/Phi-4-mini-instruct\"\n",
    "#model_path = \"./models/phi4-1e\"\n",
    "llm = LLM(model=model_path)\n",
    "sampling_params = SamplingParams(max_tokens=256)\n",
    "\n",
    "def build_prompt(claim, abstract):\n",
    "    return (\n",
    "        \"<|im_start|>system<|im_sep|>You are a professional fact checker.\"\n",
    "        \"You get a claim and an abstract of a scientific paper. Assess if the claim is supported or refuted by the abstract! Return only your verdict! Either 'Supports', 'Refutes' or 'Not Enough Information'. <|im_end|>\"\n",
    "        f\"<|im_start|>user<|im_sep|>The claim: {claim}\\nThe abstract: {abstract}\\nYour verdict: <|im_end|>\"\n",
    "        \"<|im_start|>assistant<|im_sep|>\"\n",
    "    )\n",
    "\n",
    "df = pd.read_parquet(\"climatecheck_publications_corpus.parquet\")\n",
    "\n",
    "abstracts = df[\"abstract\"].fillna(\"\").astype(str).tolist()\n",
    "batch_size = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def predict_label(pred):\n",
    "    print(pred)\n",
    "    label = \"Not Enough Information\"\n",
    "    if \"support\" in pred.lower():\n",
    "        label = \"Supports\"\n",
    "    if \"refute\" in pred.lower():\n",
    "        label = \"Refutes\"\n",
    "    if \"not enough\" in pred.lower():\n",
    "        label = \"Not Enough Information\"\n",
    "    return label\n",
    "\n",
    "predictions_df = pd.read_csv(\"./predictions_best.csv\")\n",
    "test_df = pd.read_parquet(\"test-00000-of-00001.parquet\")\n",
    "abstracts_df = pd.read_parquet(\"climatecheck_publications_corpus.parquet\")\n",
    "\n",
    "claim_map = test_df.set_index(\"claim_id\")[\"claim\"].to_dict()\n",
    "abstract_map = abstracts_df.set_index(\"abstract_id\")[\"abstract\"].to_dict()\n",
    "\n",
    "all_labels = []\n",
    "start_time = time.time()\n",
    "for _, row in tqdm(predictions_df.iterrows(), total=len(predictions_df), desc=\"Predicting labels\"):\n",
    "    claim = claim_map[row[\"claim_id\"]]\n",
    "    abstract = abstract_map[row[\"abstract_id\"]]\n",
    "    prompt = build_prompt(claim, abstract)\n",
    "    output = llm.generate([prompt], sampling_params)\n",
    "    label = predict_label(output[0].outputs[0].text.strip())\n",
    "    all_labels.append(label)\n",
    "\n",
    "end_time = time.time()\n",
    "time_delta_seconds = end_time - start_time\n",
    "print(f\"Time delta (seconds): {time_delta_seconds}\")\n",
    "filename = \"time_log_phi.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {start_time}\\n\")\n",
    "    f.write(f\"End Time: {end_time}\\n\")\n",
    "    f.write(f\"Time Delta (seconds): {time_delta_seconds}\\n\")\n",
    "\n",
    "predictions_df[\"label\"] = all_labels\n",
    "f_name = \"predictions_new_phi4_2.csv\"\n",
    "predictions_df.to_csv(f_name, index=False)\n",
    "print(f\"Saved predictions with labels to {f_name}.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee538c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Run inference with Qwen3 models\"\"\"\n",
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import time\n",
    "\n",
    "#model_name = \"Qwen/Qwen3-14B\"\n",
    "#model_name = \"Qwen/Qwen3-8B\"\n",
    "model_name = \"Qwen/Qwen3-1.7B\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side=\"left\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "reasoning=False\n",
    "no_think_tag = \"/no_think \"\n",
    "if reasoning:\n",
    "    no_think_tag = \"\"\n",
    "\n",
    "def build_prompt(claim, abstract):\n",
    "    return (\n",
    "        f\"{no_think_tag}You are a professional fact checker.\"\n",
    "        \" You get a claim and an abstract of a scientific paper.\"\n",
    "        \" Assess if the claim is supported or refuted by the abstract!\"\n",
    "        \" Return only your verdict! Either 'Supports', 'Refutes' or 'Not Enough Information'.\"\n",
    "        f\"\\n\\nThe claim: {claim}\\nThe abstract: {abstract}\\nYour verdict:\"\n",
    "    )\n",
    "\n",
    "def predict_label(pred):\n",
    "    pred = pred.lower()\n",
    "    if \"support\" in pred:\n",
    "        return \"Supports\"\n",
    "    if \"refute\" in pred:\n",
    "        return \"Refutes\"\n",
    "    if \"not enough\" in pred:\n",
    "        return \"Not Enough Information\"\n",
    "    return \"Not Enough Information\"\n",
    "\n",
    "predictions_df = pd.read_csv(\"./predictions_best.csv\")\n",
    "test_df = pd.read_parquet(\"test-00000-of-00001.parquet\")\n",
    "abstracts_df = pd.read_parquet(\"climatecheck_publications_corpus.parquet\")\n",
    "\n",
    "claim_map = test_df.set_index(\"claim_id\")[\"claim\"].to_dict()\n",
    "abstract_map = abstracts_df.set_index(\"abstract_id\")[\"abstract\"].to_dict()\n",
    "\n",
    "all_labels = []\n",
    "\n",
    "batch_size = 1\n",
    "prompts = []\n",
    "rows = []\n",
    "\n",
    "#torch.cuda.empty_cache()\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "for _, row in tqdm(predictions_df.iterrows(), total=len(predictions_df), desc=\"Preparing prompts\"):\n",
    "    claim = claim_map[row[\"claim_id\"]]\n",
    "    abstract = abstract_map[row[\"abstract_id\"]]\n",
    "    prompt = build_prompt(claim, abstract)\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    chat_text = tokenizer.apply_chat_template(\n",
    "        messages,\n",
    "        tokenize=False,\n",
    "        add_generation_prompt=True,\n",
    "        enable_thinking=True\n",
    "    )\n",
    "    prompts.append(chat_text)\n",
    "    rows.append(row)\n",
    "\n",
    "    if len(prompts) == batch_size:\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "            outputs = model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=512\n",
    "            )\n",
    "\n",
    "        for i in range(batch_size):\n",
    "            output_ids = outputs[i][len(inputs[\"input_ids\"][i]):].tolist()\n",
    "            try:\n",
    "                index = len(output_ids) - output_ids[::-1].index(tokenizer.convert_tokens_to_ids(\"</think>\"))\n",
    "            except ValueError:\n",
    "                index = 0\n",
    "            content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "            label = predict_label(content)\n",
    "            all_labels.append(label)\n",
    "\n",
    "        prompts.clear()\n",
    "        rows.clear()\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "if prompts:\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=512\n",
    "        )\n",
    "\n",
    "    for i in range(len(prompts)):\n",
    "        output_ids = outputs[i][len(inputs[\"input_ids\"][i]):].tolist()\n",
    "        try:\n",
    "            index = len(output_ids) - output_ids[::-1].index(tokenizer.convert_tokens_to_ids(\"</think>\"))\n",
    "        except ValueError:\n",
    "            index = 0\n",
    "        content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "        label = predict_label(content)\n",
    "        all_labels.append(label)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "end_time = time.time()\n",
    "time_delta_seconds = end_time - start_time\n",
    "print(f\"Time delta (seconds): {time_delta_seconds}\")\n",
    "filename = f\"{model_name}_time_log.txt\"\n",
    "with open(filename, \"w\") as f:\n",
    "    f.write(f\"Start Time: {start_time}\\n\")\n",
    "    f.write(f\"End Time: {end_time}\\n\")\n",
    "    f.write(f\"Time Delta (seconds): {time_delta_seconds}\\n\")\n",
    "    \n",
    "predictions_df[\"label\"] = all_labels\n",
    "output_file = \"predictions_{model_name}.csv\"\n",
    "predictions_df.to_csv(output_file, index=False)\n",
    "print(f\"Saved predictions with labels to {output_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
